{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What would be the ideal way to do this?\n",
    "\n",
    "1. Define points $x_0,\\dots,x_{n-1}$ (arbitrary) and pick some $\\varepsilon$\n",
    "2. Define the $\\varepsilon$ graph. That is the graph where $i-j$ connect if $D[i,j] \\leq \\varepsilon$\n",
    "3. Define the VR complex at $\\varepsilon$. Every complete subgraph becomes a simplex. This is where we wonder if enumerating all simplices creates duplicates\n",
    "\n",
    "Idea: Only generate simplices with increasing vertex order.\n",
    "$$N^+(i) = \\{\\, j>i : D[i,j]\\leq \\varepsilon \\,\\}$$\n",
    "I.e. neighbors that come after me.\n",
    "\n",
    "4. Now we have a simplex $\\sigma = [i_0<i_1<\\dots<i_k]$. We want to add a new vertex $n$ to make the simplex larger. For $\\sigma \\cup \\{n\\}$ to be a simplex, $n$ must connect to every vertex in $\\sigma$ in the $\\varepsilon$ graph. \n",
    "\n",
    "$$n \\in N^+(i_0)\\cap N^+(i_1)\\cap \\cdots \\cap N^+(i_k)$$\n",
    "\n",
    "This is why we should maintain a candidate set $C(\\sigma) = N^+(i_0)\\cap N^+(i_1)\\cap \\cdots \\cap N^+(i_k)$. Now we start at vertex $i$ and find $C([i]) = N^+(i)$. If we extend by adding n, the new candidates become $C(\\sigma \\cup \\{n\\}) = C(\\sigma)\\cap N^+(n)$.\n",
    "\n",
    "This guarantees that only vertices that connect to all simplex vertices are considered and they respect ordering.\n",
    "\n",
    "5. If we were to compute all edges, then all triangles, ..., we would store too many simplices. We want to build one simplex at a time, emit its contribution as soon as possible, then backtrack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does Dlotko do compared to this?\n",
    "\n",
    "1. We already define points in an ascending arbitrary manner with this.\n",
    "```python\n",
    "def pairwise_dist(X: np.ndarray) -> np.ndarray: #input is a shape X with n points and d dim\n",
    "    diff = X[:, None, :] - X[None, :, :] #diff is every pairwise distance vector\n",
    "    return np.linalg.norm(diff, axis=2)\n",
    "\n",
    "def subseq_neighbors(D: np.ndarray, i: int, epsilon: float) -> list[int]:\n",
    "    n = D.shape[0] #precomputed distance matrix D (n is number of points)\n",
    "    js = np.where((D[i] <= epsilon) & (np.arange(n) > i))[0]\n",
    "    return js.tolist()\n",
    "```\n",
    "subseq_neighbors returns the neighbor set of a vertex with only indices larger than itself ($N^+(i) = \\{\\, j > i \\mid D[i,j] \\leq \\varepsilon \\,\\}$). This is how we enforce ordering and avoid duplicate simplices. Let's analyze the line `js = np.where((D[i] <= epsilon) & (np.arange(n) > i))[0]`. `D[i] <= epsilon` is taking the ith row of the distance matrix produces a boolean array containing True is point j is withing $\\varepsilon$ of i. `np.arange(n) > 1` creates $[0,1, \\dots, n-1]$ and produces a boolean list of True if the indice in greater than i. Now we take the and of these two state ments so we have a boolean list that says True if the point is within $\\varepsilon$ distance and its index is $> i$. `np.where` returns the indices where the condition is True, so js is an array of valid neighbors of vertex $i$. \n",
    "\n",
    "pairwise_dist takes in a shape $X$ and outputs its symmetric distance matrix $D[i,j] = \\|X[i] - X[j]\\|$. We can have some issues here with large VR. If $n=10,000$, then $D$ contains 100,000,000 entries. Potentially think about avoiding storing the full distance matrix. \n",
    "\n",
    "2. Looking at the Algorithms\n",
    "\n",
    "Abstractly what we want to do is:\n",
    "\n",
    "```python\n",
    "for each i:\n",
    "    expand([i], 0, N+(i))\n",
    "```\n",
    "\n",
    "We currently have:\n",
    "\n",
    "```python\n",
    "for i in range(n):\n",
    "    simplices = [[i]] #this is a list of a list just like a simplex is a set of a set (this is initializing our simplex)\n",
    "    filtrations = [0]\n",
    "    common_subseq_neighs = subseq_neighbors(D, i, epsilon) #this is C = N+(i)\n",
    "```\n",
    "These two are identical.\n",
    "\n",
    "\n",
    "Now we want to `emit(fsigma, (-1)^dim)` and we have:\n",
    "\n",
    "```python\n",
    "for sigma, f_sigma in zip(simplices, filtrations):\n",
    "    dim_sigma = len(sigma) - 1\n",
    "    sign = 1 if (dim_sigma % 2 == 0) else -1 #this is for EC assigns its sign in the alternating sum\n",
    "    C.append((f_sigma, sign)) #stores a pair (filtration value, +-1)\n",
    "```\n",
    "\n",
    "Now we wish to grow the simplex. \n",
    "\n",
    "Ideally:\n",
    "```python\n",
    "for n in C:\n",
    "    f_new = max(fsigma, max_{v in sigma} D[v,n])\n",
    "    C_new = C intersect N+(n)\n",
    "```\n",
    "We have:\n",
    "```python\n",
    "for sigma, f_sigma, commonN in zip(simplices, filtrations, common_subseq_neighs):\n",
    "    for n in commonN:\n",
    "        sigma2 = sigma + [n]\n",
    "```\n",
    "This corresponds to item number 4 on the top block.\n",
    "\n",
    "Now we need to update the filtration for this new simplex ($\\sigma \\cap \\{n\\}$). Ideally in pseudo code this is `f_new = max(fsigma, longest_edge)` We have:\n",
    "\n",
    "```python\n",
    "longest_edge = 0.0\n",
    "for v in sigma:\n",
    "    d = D[v,n]\n",
    "    if d > longest_edge:\n",
    "        longest_edge = d\n",
    "new_f = max(f_sigma, longest_edge)\n",
    "``` \n",
    "This is the same mathematically as the pseudocode.\n",
    "\n",
    "After this we need to update the candidate set `C_new = C intersect N+(n)`. We have:\n",
    "\n",
    "```python\n",
    "neigh_n = subseq_neighbors(D, n, epsilon)\n",
    "intersection = sorted(set(commonN).intersection(neigh_N))\n",
    "```\n",
    "which is the same mathematically as well.\n",
    "\n",
    "Now we hit a difference!\n",
    "\n",
    "Ideally we build one simplex chain as deep as possible then backtrack like:\n",
    "```python\n",
    "expand(i):\n",
    "    expand(i,j):\n",
    "        expand(i,j,k):\n",
    "            expand(i,j,k,l):\n",
    "                ...\n",
    "```\n",
    "This stores memory in terms of O(max simplex dim) not O(simplex number)\n",
    "\n",
    "We currently have\n",
    "```python\n",
    "while simplices not empty:\n",
    "    record all simplices at this dimension\n",
    "    simplices = inc_dim(...)\n",
    "```\n",
    "\n",
    "This iterates all vertices, then edges starting at i, then triangles at i, ... We have a memory risk in the logic here.\n",
    "\n",
    "Another place we have a memory risk is that we recompute `neigh_n = subseq_neighbors(D, n ,epsilon)` every time we extend the simplex, which is redundant. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try to Optimize this sampling from [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def pairwise_dist(X: np.ndarray) -> np.ndarray:\n",
    "    diff = X[:, None, :] - X[None, :, :]\n",
    "    return np.linalg.norm(diff, axis = 2)\n",
    "#this is the same as before inputs shape X and outputs symmetric distance matrix D\n",
    "\n",
    "def Nplus(D: np.ndarray, epsilon: float) -> list[np.ndarray]:\n",
    "    #precompute Nplus[i] = sorted array of neighbors j > i with D[i,j] <= epsilon\n",
    "    n = D.shape[0] #number of vertices\n",
    "    N = []\n",
    "    for i in range(n):\n",
    "        js = np.where(D[i, i + 1:] <= epsilon)[0] + (i + 1)\n",
    "        N.append(js.astype(np.int32))\n",
    "    return N #prevents double counting and returns the N+ matrix for all vertices\n",
    "\n",
    "def intersect_sorted(a: np.ndarray, b: np.ndarray) -> np.ndarray:\n",
    "    #intersection of two sorted integer arrays with numpys intersect1d\n",
    "    if a.size == 0 or b.size == 0:\n",
    "        return np.empty(0, dtype = np.int32)\n",
    "    return np.intersect1d(a, b, assume_unique=False)\n",
    "    # this is how we enforce that future vertices must neighbor all current simplex vertices\n",
    "\n",
    "\n",
    "def local_contributions_vr_dfs( #enumerating all simplices in the epsilon graph up to some max_dim\n",
    "        X: np.ndarray,\n",
    "        epsilon: float,\n",
    "        max_dim: int | None = None,\n",
    "        ) -> list[tuple[float, int]]:\n",
    "    \"\"\"\n",
    "    Using clean DFS complete subgraph enumeration in the epsilon graph with ordered neighbor sets.\n",
    "    Emits (filtration_value, (-1)^dim) for each simplex found to save memory.\n",
    "    outputs C which is a list of events at filtration value f add 1 or subtract 1\n",
    "    \"\"\"\n",
    "    X = np.asarray(X, dtype = float)\n",
    "    n = X.shape[0]\n",
    "    D = pairwise_dist(X)\n",
    "    N = Nplus(D, epsilon)\n",
    "\n",
    "    C: list[tuple[float, int]] = [] # candidate set is a set of float,int tuples with (filtration_val, sign)\n",
    "    \n",
    "    def expand(simplex: list[int], candidates: np.ndarray, f_simplex: float):\n",
    "        #simplex is a list of vertex indices in sigma. Candidates is C which is the allowed vertices to add next (the intersection set).\n",
    "        # f_simplex is the current filtration value for sigma (max edge in sigma so far). \n",
    "\n",
    "        #emit contribution of the current simplex for memory\n",
    "        dim = len(simplex) - 1\n",
    "        sign = 1 if (dim % 2 == 0) else -1\n",
    "        C.append((f_simplex, sign))\n",
    "\n",
    "        #dimension cap (don't expand further than max_dim)\n",
    "        if max_dim is not None and dim >= max_dim:\n",
    "            return\n",
    "        \n",
    "        #now try expanding by each candidate vertex v\n",
    "        for v in candidates:\n",
    "            v = int(v)\n",
    "            #update filtration with max of current filtration and longest edge to v\n",
    "            longest_edge = 0.0\n",
    "            for u in simplex:\n",
    "                d = D[u, v]\n",
    "                if d > longest_edge:\n",
    "                    longest_edge = d\n",
    "            f_new = max(f_simplex, longest_edge)\n",
    "            # computes max(f_sigma, maxD[u,v] for u in sigma)\n",
    "\n",
    "            #update candidate set (C intersect N+[v] = C(sigma union {v}))\n",
    "            candidates_new = intersect_sorted(candidates, N[v])\n",
    "\n",
    "            simplex.append(v) #add v\n",
    "            expand(simplex, candidates_new, f_new)\n",
    "            simplex.pop()\n",
    "    \n",
    "    #start from each vertex i as the smallest vertex index\n",
    "    for i in range(n):\n",
    "        expand([i], N[i], 0.0)\n",
    "    \n",
    "    C.sort(key = lambda t: t[0]) #sort by filtration value to to cumulatibe sums in order\n",
    "    return C\n",
    "\n",
    "def EC_at_eps(C: list[tuple[float, int]], r: float) -> int:\n",
    "    #C must be sorted by filtration value (which it is)\n",
    "    total = 0\n",
    "    for f, s in C:\n",
    "        if f <= r:\n",
    "            total += s\n",
    "        else:\n",
    "            break\n",
    "    return total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What now?\n",
    "\n",
    "Now we have C, our total candidate matrix containing tuples of (filtration value, sign) for all vertices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 -8463\n",
      "0.25 -159404\n",
      "0.4 -611852\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "X = np.random.rand(n,1)\n",
    "eps_enum = 0.5\n",
    "\n",
    "C = local_contributions_vr_dfs(X, eps_enum, max_dim=3)\n",
    "\n",
    "for eps in [0.1, 0.25, 0.4]:\n",
    "    print(eps, EC_at_eps(C, eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "def simulate_EC_dist(\n",
    "        eps_list,\n",
    "        n = 200,\n",
    "        d = 1,\n",
    "        trials = 30,\n",
    "        max_dim = 30,\n",
    "        seed = 0\n",
    "        ):\n",
    "    \"\"\"\n",
    "    for each epsilon in eps_list sample X ~ unif([0,1]^d) trials times. \n",
    "    compute C using eps_enum = epsilon\n",
    "    compute EC at r = epsilon\n",
    "    returns dict eps -> np.array of EC values (length = trials)\n",
    "    \"\"\"\n",
    "    rng = np.random.default_rng(seed)\n",
    "    results = {}\n",
    "\n",
    "    for eps in eps_list:\n",
    "        ecs = []\n",
    "        for _ in range(trials):\n",
    "            X = rng.random((n,d)) #uniform([0,1]^d)\n",
    "\n",
    "            C = local_contributions_vr_dfs(X, eps, max_dim=max_dim)\n",
    "            ec = EC_at_eps(C, eps)\n",
    "            ecs.append(ec)\n",
    "\n",
    "        results[eps] = np.array(ecs, dtype= int)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1 mean 356.8666666666667 std 82.01046003745836 min 244 max 526\n",
      "0.25 mean 2564.5333333333333 std 321.4607008986047 min 1988 max 3229\n",
      "0.4 mean 6219.4 std 905.3609077784026 min 4693 max 8317\n"
     ]
    }
   ],
   "source": [
    "eps_list = [0.1, 0.25, 0.4]\n",
    "results = simulate_EC_dist(eps_list, n=50, d=1, trials=30, max_dim=2, seed=42)\n",
    "\n",
    "for eps in eps_list:\n",
    "    vals = results[eps]\n",
    "    print(eps, \"mean\", vals.mean(), \"std\", vals.std(), \"min\", vals.min(), \"max\", vals.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01 39\n",
      "0.06210526315789474 -1694\n",
      "0.11421052631578947 -14328\n",
      "0.16631578947368422 -44357\n",
      "0.21842105263157896 -99060\n",
      "0.2705263157894737 -201135\n",
      "0.32263157894736844 -334935\n",
      "0.37473684210526315 -516044\n",
      "0.4268421052631579 -698684\n",
      "0.4789473684210527 -943645\n",
      "0.5310526315789474 -1041976\n",
      "0.5831578947368421 -1041976\n",
      "0.6352631578947369 -1041976\n",
      "0.6873684210526316 -1041976\n",
      "0.7394736842105263 -1041976\n",
      "0.791578947368421 -1041976\n",
      "0.8436842105263158 -1041976\n",
      "0.8957894736842106 -1041976\n",
      "0.9478947368421053 -1041976\n",
      "1.0 -1041976\n"
     ]
    }
   ],
   "source": [
    "for r in np.linspace(0.01, 1.0, 20):\n",
    "    ec = EC_at_eps(C, r)\n",
    "    print(r, ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
